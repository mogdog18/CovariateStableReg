{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant LogisticRegression. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant LinearRegression. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant accuracy_score. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant precision_score. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant recall_score. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant mean_squared_error. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant LogisticRegression. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant accuracy_score. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant precision_score. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant recall_score. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "using CSV, DataFrames, Statistics, Random, Plots, JuMP, Gurobi, Distributions, Random, Metrics\n",
    "using StatsBase: sample\n",
    "\n",
    "#import sklearn functions \n",
    "using ScikitLearn\n",
    "@sk_import linear_model: LogisticRegression\n",
    "@sk_import linear_model: LinearRegression\n",
    "@sk_import metrics:accuracy_score;\n",
    "@sk_import metrics:precision_score;\n",
    "@sk_import metrics:recall_score;\n",
    "@sk_import metrics:mean_squared_error;\n",
    "\n",
    "include(\"utils.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo change X_shifted that it is added later\n",
    "mutable struct RegressionClass\n",
    "    name::String\n",
    "    X::DataFrame\n",
    "    X_shifted::DataFrame\n",
    "    y::Vector{Float64}\n",
    "    covar_parameters::Vector{Any}\n",
    "    covar_dist_type::String\n",
    "    lambas_range::Vector{Float64}\n",
    "    train_test_prop::Float64\n",
    "    train_val_prop::Float64\n",
    "    num_runs::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_covariate_shift (generic function with 2 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_covariate_shift(rc::RegressionClass, seed = 123)\n",
    "    #set a seed for distribution\n",
    "    Random.seed!(seed)\n",
    "\n",
    "    if rc.covar_dist_type == \"Normal\"\n",
    "        col_means = mean(Matrix(rc.X), dims=1)\n",
    "\n",
    "        random_values = zeros(size(rc.X))\n",
    "\n",
    "        for col_index in 1:size(rc.X, 2)\n",
    "            col_mean = col_means[col_index]\n",
    "            col_shift = rand(Normal(col_mean * rc.covar_parameters[1] , rc.covar_parameters[2]), (size(rc.X, 1), 1))\n",
    "            random_values[:, col_index] = col_shift\n",
    "        end\n",
    "\n",
    "        X_shifted = Matrix(rc.X) .+ random_values;\n",
    "        X_shifted_df = DataFrame(X_shifted, Symbol.(names(rc.X)))\n",
    "    end\n",
    "\n",
    "    rc.X_shifted = X_shifted_df \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_test_split (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_test_split(rc::RegressionClass, random_seed)\n",
    "    \n",
    "    Random.seed!(random_seed)\n",
    "\n",
    "    num_indices = round(Int, rc.train_test_prop * length(rc.y))\n",
    "    train_indices = sample(1:length(rc.y), num_indices, replace=false)\n",
    "    test_indices = setdiff(1:length(rc.y), train_indices)\n",
    "    \n",
    "    X_train, y_train = Matrix(rc.X)[train_indices, :], rc.y[train_indices]\n",
    "    X_test, y_test = Matrix(rc.X_shifted)[test_indices, :], rc.y[test_indices]\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize_data (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function normalize_data(rc::RegressionClass, X_train, X_test)\n",
    "    # Calculate mean and standard deviation from training data\n",
    "    mean_vals = mean(X_train, dims=1)\n",
    "    std_vals = std(X_train, dims=1)\n",
    "\n",
    "    # Normalize training data\n",
    "    X_train_norm = (X_train .-mean_vals) ./ std_vals;\n",
    "    X_test_norm = (X_test .-mean_vals) ./ std_vals;\n",
    "    \n",
    "    return X_train_norm, X_test_norm\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_val_opt_split (generic function with 2 methods)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_val_opt_split(rc::RegressionClass, X_train_full, y_train_full, beta_opt, weights = \"nothing\")\n",
    "\n",
    "    residuals = y_train_full - X_train_full * beta_opt\n",
    "\n",
    "    if weights == \"nothing\"\n",
    "        sorted_indices = sortperm(abs.(residuals), rev=true)\n",
    "    else \n",
    "        residuals_weights = [residuals[i] * weights[i] for i in 1:length(weights)]\n",
    "        sorted_indices = sortperm(abs.(residuals_weights), rev=true)\n",
    "    end\n",
    "\n",
    "    num_train_points = round(Int, rc.train_val_prop * length(sorted_indices))\n",
    "\n",
    "    train_indices = sorted_indices[1:num_train_points]\n",
    "\n",
    "    val_indices = setdiff(1:length(y_train_full), train_indices)\n",
    "\n",
    "    X_train = X_train_full[train_indices, :] \n",
    "    y_train = y_train_full[train_indices]\n",
    "\n",
    "    X_val = X_train_full[val_indices, :]\n",
    "    y_val = y_train_full[val_indices]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_optimized_split (generic function with 2 methods)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todo: could change X, y to X_train, y_train\n",
    "function get_optimized_split(rc::RegressionClass, X, y, lambda, weights=\"nothing\")\n",
    "    # to do: should n be an attribute\n",
    "    n, p = size(X)\n",
    "    k = round(Int, n * rc.train_val_prop)\n",
    "    \n",
    "    model = Model(Gurobi.Optimizer)\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "    \n",
    "    @variable(model, theta)\n",
    "    @variable(model, u[1:n] >= 0)\n",
    "    @variable(model, beta[1:p])\n",
    "    @variable(model, w[1:p])\n",
    "    @objective(model, Min, k * theta + sum(u) + lambda * sum(w))\n",
    "    \n",
    "    for i in 1:p\n",
    "        @constraint(model, w[i] >= beta[i])\n",
    "        @constraint(model, w[i] >= -beta[i])\n",
    "    end \n",
    "    \n",
    "    for i in 1:n\n",
    "        if weights == \"nothing\"\n",
    "            @constraint(model, theta + u[i] >= y[i] - sum(X[i, :].*beta))\n",
    "            @constraint(model, theta + u[i] >= -(y[i] - sum(X[i, :].*beta)))\n",
    "        else \n",
    "            @constraint(model, theta + u[i] >=  weights[i] * (y[i] - sum(X[i, :].*beta)))\n",
    "            @constraint(model, theta + u[i] >= - weights[i] * (y[i] - sum(X[i, :].*beta)))\n",
    "        end \n",
    "    end\n",
    "    \n",
    "    for i in 1:n\n",
    "    weight_term = (weights == \"nothing\") ? 1 : weights[i]\n",
    "    @constraint(model, theta + u[i] >= weight_term * (y[i] - sum(X[i, :].*beta)))\n",
    "    @constraint(model, theta + u[i] >= -weight_term * (y[i] - sum(X[i, :].*beta)))\n",
    "    end\n",
    "    \n",
    "    optimize!(model)\n",
    "    return value(theta), value.(u), value.(beta), value.(w) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_optimized_split_test_score (generic function with 3 methods)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_optimized_split_test_score(rc::RegressionClass, X_full_train, y_full_train, X_test, y_test, weights=\"nothing\", print_result=false)\n",
    "    best_lambda = Inf\n",
    "    best_val_mse = Inf\n",
    "    best_model = Inf\n",
    "    \n",
    "    for lambda in rc.lambas_range\n",
    "        # Get optimized split\n",
    "        _, _, betas, _ = get_optimized_split(rc, X_full_train, y_full_train, lambda, weights)\n",
    "        X_train, y_train, X_val, y_val = train_val_opt_split(rc, X_full_train, y_full_train, betas, weights)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred_val =  X_val * betas\n",
    "        val_mse_i = mse(y_val, y_pred_val)\n",
    "        \n",
    "        if best_val_mse > val_mse_i\n",
    "            best_lambda = lambda\n",
    "            best_val_mse = val_mse_i\n",
    "            best_model = betas\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #get mse on test set for best performing model \n",
    "    y_pred_test = X_test * best_model\n",
    "    mse_test_mse = mse(y_test, y_pred_test)\n",
    "\n",
    "    if print_result\n",
    "        println(\"Best lambda: \", best_lambda)\n",
    "        println(\"Validation score: \", best_val_mse)\n",
    "        println(\"Test score: \", mse_test_mse)\n",
    "        println(\"Number of betas: \", length(best_model))\n",
    "    end\n",
    "\n",
    "    return mse_test_mse\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_random_test_score (generic function with 3 methods)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_random_test_score(rc::RegressionClass, X_full_train, y_full_train, X_test, y_test, seed_value, weights=\"nothing\", print_result=false)\n",
    "    best_lambda = Inf\n",
    "    best_val_mse = Inf\n",
    "    best_model = Inf\n",
    "\n",
    "    (X_train, y_train), (X_val, y_val) = \n",
    "        IAI.split_data(:regression, X_full_train, y_full_train, train_proportion=rc.train_val_prop, seed=seed_value)\n",
    "\n",
    "    for lambda in rc.lambas_range\n",
    "        beta_star = LassoRegression(X_train, y_train, lambda, weights)\n",
    "        y_pred_val = X_val * beta_star\n",
    "        val_mse_i = mse(y_val, y_pred_val)\n",
    "\n",
    "        if best_val_mse > val_mse_i\n",
    "            best_lambda = lambda\n",
    "            best_val_mse = val_mse_i\n",
    "            best_model = beta_star\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #get test mse score on best performing model\n",
    "    y_pred_test = X_test * best_model\n",
    "    mse_test_mse = mse(y_test,y_pred_test)\n",
    "\n",
    "    if print_result\n",
    "        println(\"Best lambda: \", best_lambda)\n",
    "        println(\"Validation score: \", best_val_mse)\n",
    "        println(\"Test score: \", mse_test_mse)\n",
    "        println(\"Number of betas: \", length(best_model.coef_) + length(best_model.intercept_))\n",
    "    end\n",
    "\n",
    "    return mse_test_mse\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repeat_four_methods (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todo: deal with prints (false)\n",
    "#todo: add what is returned to instance \n",
    "function repeat_four_methods(rc::RegressionClass)\n",
    "\n",
    "    random_mse_test_scores, random_weights_mse_test_scores = [], []\n",
    "    optim_mse_test_scores, optim_weights_mse_test_scores = [], []\n",
    "\n",
    "    for random_seed in 1:rc.num_runs\n",
    "        random_seed = Int(random_seed) #need to be int \n",
    "        \n",
    "        #split and normalise data\n",
    "        (X_train, y_train), (X_test, y_test) = train_test_split(rc, random_seed)\n",
    "        X_train_norm, X_test_norm = normalize_data(rc, X_train, X_test)\n",
    "        X_train_norm, X_test_norm = add_intercept(X_train_norm), add_intercept(X_test_norm)\n",
    "        \n",
    "        weights = get_weights(X_train_norm, X_test_norm, false)\n",
    "\n",
    "        if random_seed % 10 == 0\n",
    "            println(random_seed)\n",
    "        end\n",
    "        \n",
    "        println(\"Starting Randomization\")\n",
    "        random_mse_test_score = get_random_test_score(rc, X_train_norm, y_train, X_test_norm, y_test, random_seed, \"nothing\", false)\n",
    "        push!(random_mse_test_scores, random_mse_test_score)\n",
    "        \n",
    "        println(\"Starting Randomization with covariate weights\")\n",
    "        random_weights_mse_test_score = get_random_test_score(rc, X_train_norm, y_train, X_test_norm, y_test, random_seed, weights, false)\n",
    "        push!(random_weights_mse_test_scores, random_weights_mse_test_score)\n",
    "        \n",
    "        println(\"Starting Optimization\")\n",
    "        optim_mse_test_score = get_optimized_split_test_score(rc, X_train_norm, y_train, X_test_norm, y_test, \"nothing\", false)\n",
    "        push!(optim_mse_test_scores, optim_mse_test_score)\n",
    "        \n",
    "        println(\"Starting Optimization with covariate weights\")\n",
    "        optim_weights_mse_test_score = get_optimized_split_test_score(rc, X_train_norm, y_train, X_test_norm, y_test, weights, false)\n",
    "        push!(optim_weights_mse_test_scores, optim_weights_mse_test_score)\n",
    "    end \n",
    "    \n",
    "    print(\"hello\")\n",
    "    return random_mse_test_scores, random_weights_mse_test_scores, optim_mse_test_scores, optim_weights_mse_test_scores\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "train_fraction = 0.7 \n",
    "test_fraction = 0.9\n",
    "lambdas = [0.00001, 0.0001, 0.001, 0.01, 0.1];\n",
    "\n",
    "X_full, y_full = get_concrete_data()\n",
    "test_instance = RegressionClass(\"test\", X_full, X_full, y_full, [0.5, 1.0], \"Normal\", lambdas, test_fraction, train_fraction, num_runs);\n",
    "#todo: call during instantiation \n",
    "generate_covariate_shift(test_instance);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Randomization\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Starting Randomization with covariate weights\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Starting Optimization\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Starting Optimization with covariate weights\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n",
      "hello% improvement in mse with no weight: 52.19096579979966\n",
      "% improvement in mse with weight: 95.75624952598213\n"
     ]
    }
   ],
   "source": [
    "random_mse_test_scores, random_weights_mse_test_scores, optim_mse_test_scores, optim_weights_mse_test_scores = repeat_four_methods(test_instance)\n",
    "\n",
    "improv_no_weight = calculate_percentage_improvement(optim_mse_test_scores, random_mse_test_scores)\n",
    "improv_with_weight = calculate_percentage_improvement(optim_weights_mse_test_scores, random_weights_mse_test_scores)\n",
    "\n",
    "println(\"% improvement in mse with no weight: \", improv_no_weight)\n",
    "println(\"% improvement in mse with weight: \", improv_with_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = train_test_split(test_instance, 1);\n",
    "X_train_norm, X_test_norm = normalize_data(test_instance, X_train, X_test);\n",
    "X_train_norm, X_test_norm = add_intercept(X_train_norm), add_intercept(X_test_norm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (IAI) 4 1.9.3",
   "language": "julia",
   "name": "julia-_iai_-4-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "683a8f5228cdb2dfe20440e7c79750b7ef6077ddd22e2c437eb1bcaa2db9b8fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
