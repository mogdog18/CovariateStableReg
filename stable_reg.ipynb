{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Random, Plots, JuMP, Gurobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant LinearRegression. This may fail, cause incorrect answers, or produce other errors.\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule model_selection has been ported to Julia - try `import ScikitLearn: CrossValidation` instead\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ ScikitLearn.Skcore ~/.julia/packages/ScikitLearn/sqLdT/src/Skcore.jl:259\u001b[39m\n",
      "WARNING: redefinition of constant train_test_split. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <function train_test_split at 0x2ab26dea0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "@sk_import linear_model: LinearRegression\n",
    "@sk_import linear_model: Lasso\n",
    "@sk_import model_selection: train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opt_split (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function opt_split(X, y, n, p, k, lambda, weight = nothing)\n",
    "    # add column of ones\n",
    "    X = hcat(ones(Int, size(X, 1)), X)\n",
    "    p = p + 1\n",
    "\n",
    "    model = Model(Gurobi.Optimizer)\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    @variable(model, theta)\n",
    "    @variable(model, u[1:n] >= 0)\n",
    "    @variable(model, beta[1:p])\n",
    "    @variable(model, w[1:p])\n",
    "\n",
    "    @objective(model, Min, k * theta + sum(u) + lambda * sum(w))\n",
    "\n",
    "    for i in 1:p\n",
    "        @constraint(model, w[i] >= beta[i])\n",
    "        @constraint(model, w[i] >= -beta[i])\n",
    "    end \n",
    "\n",
    "    for i in 1:n\n",
    "        if weight == nothing\n",
    "            @constraint(model, theta + u[i] >= y[i] - sum(X[i, :].*beta))\n",
    "            @constraint(model, theta + u[i] >= -(y[i] - sum(X[i, :].*beta)))\n",
    "        else \n",
    "            @constraint(model, theta + u[i] >=  weight[i] * (y[i] - sum(X[i, :].*beta)))\n",
    "            @constraint(model, theta + u[i] >= - weight[i] * (y[i] - sum(X[i, :].*beta)))\n",
    "        end \n",
    "    end\n",
    "    \n",
    "    optimize!(model)\n",
    "\n",
    "    return value(theta), value.(u), value.(beta), value.(w)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split_train_val (generic function with 3 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function split_train_val(X, y, beta_star, train_fraction = 0.8, weights = nothing)\n",
    "    # add column of ones\n",
    "    X = hcat(ones(Int, size(X, 1)), X)\n",
    "\n",
    "    residuals = y - X * beta_star\n",
    "\n",
    "    if weights == nothing\n",
    "        sorted_indices = sortperm(abs.(residuals), rev=true)\n",
    "    else \n",
    "        residuals_weights = [residuals[i] * weights[i] for i in 1:length(weights)]\n",
    "        sorted_indices = sortperm(abs.(residuals_weights), rev=true)\n",
    "    end\n",
    "\n",
    "    num_train_points = round(Int, train_fraction * length(sorted_indices))\n",
    "\n",
    "    train_indices = sorted_indices[1:num_train_points]\n",
    "\n",
    "    val_indices = setdiff(1:length(y), train_indices)\n",
    "\n",
    "    X_train = X[train_indices, 2:end]\n",
    "    y_train = y[train_indices, :]\n",
    "\n",
    "    X_val = X[val_indices, 2:end]\n",
    "    y_val = y[val_indices, :]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_evaluate_model (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fit_evaluate_model(X_train, X_val, X_test, y_train, y_val, y_test, lambdas)\n",
    "    current_lambda = Inf\n",
    "    current_val_mse = Inf\n",
    "    current_test_mse = Inf\n",
    "    \n",
    "    for lambda in lambdas\n",
    "        model = Lasso(alpha=lambda)\n",
    "        fit!(model, X_train, y_train)\n",
    "        \n",
    "        y_pred_val = predict(model, X_val)\n",
    "        val_mse_i = mse(y_val, y_pred_val)\n",
    "        \n",
    "        if current_val_mse >= val_mse_i\n",
    "            current_lambda = lambda\n",
    "            current_val_mse = val_mse_i\n",
    "            current_test_mse = mse(y_test, predict(model, X_test))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    println(\"best lambda: \", current_lambda)\n",
    "    println(\"val score: \", current_val_mse)\n",
    "    println(\"test score: \", current_test_mse)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mse(y_true, y_pred)\n",
    "    mse = mean((y_true .- y_pred).^2)\n",
    "    return mse\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = CSV.read(\"abalone_original.csv\", DataFrame)\n",
    "selected_columns = setdiff(names(df), [\"rings\", \"sex\"])\n",
    "\n",
    "X = df[:,selected_columns]\n",
    "y = CSV.read(\"abalone_original.csv\", DataFrame)[!,\"rings\"];\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) =\n",
    "    IAI.split_data(:regression, X, y, train_proportion=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get optimised split \n",
    "n = nrow(train_X)\n",
    "p = ncol(train_X)\n",
    "train_fraction = 0.7\n",
    "k = n * train_fraction\n",
    "lambda = 0.001;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-09-11\n"
     ]
    }
   ],
   "source": [
    "#get optimised split\n",
    "_, _, betas_star, _ = opt_split(Array(train_X), Array(train_y), n, p, k, lambda)\n",
    "X_train_opt, y_train_opt, X_val_opt, y_val_opt= split_train_val(Array(train_X), Array(train_y), betas_star);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get non optimised split \n",
    "(X_train_noopt, y_train_noopt), (X_val_noopt, y_val_noopt) =\n",
    "    IAI.split_data(:regression, train_X, train_y, train_proportion=train_fraction);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best lambda: 1.0e-5\n",
      "val score: 0.2797520418409477\n",
      "test score: 4.944705216608463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/norahallqvist/.julia/conda/3/aarch64/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.720e+02, tolerance: 3.570e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/norahallqvist/.julia/conda/3/aarch64/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.556e+00, tolerance: 3.570e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "fit_evaluate_model(X_train_opt, X_val_opt, Matrix(test_X), y_train_opt, y_val_opt, Array(test_y), lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best lambda: 0.1\n",
      "val score: 4.946774345220674\n",
      "test score: 4.966011577831817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/norahallqvist/.julia/conda/3/aarch64/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+03, tolerance: 2.806e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/norahallqvist/.julia/conda/3/aarch64/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.024e+02, tolerance: 2.806e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/norahallqvist/.julia/conda/3/aarch64/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.690e+00, tolerance: 2.806e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "fit_evaluate_model(Matrix(X_train_noopt), Matrix(X_val_noopt), Matrix(test_X), Array(y_train_noopt), y_val_noopt, Array(test_y), lambdas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (IAI) 4 1.9.3",
   "language": "julia",
   "name": "julia-_iai_-4-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "683a8f5228cdb2dfe20440e7c79750b7ef6077ddd22e2c437eb1bcaa2db9b8fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
